# Вероятностные структуры данных — математика и реализация

## Общая идея

Классические структуры (HashSet, HashMap) хранят данные целиком — O(n) памяти, 100% точность.

Вероятностные структуры хранят **компактное представление** данных — O(m) памяти где m << n, и дают **вероятностные гарантии** вместо точных ответов.

**Фундаментальный trade-off:**
```
Память ↓  →  Точность ↓
Память ↑  →  Точность ↑
```

Все 6 алгоритмов построены на одном принципе: **хеш-функция случайно но детерминированно отображает элемент в компактное пространство**. Случайность даёт вероятностные гарантии. Детерминированность — воспроизводимость.

**Два вида ошибок:**
- **False Positive (FP):** структура говорит "есть", а элемента нет → допустимо
- **False Negative (FN):** структура говорит "нет", а элемент есть → недопустимо (кроме CMS)

Все структуры кроме Count-Min Sketch гарантируют отсутствие FN.

---

## 1. Bloom Filter

### Почему работает математически

Битовый массив из m бит, k независимых хеш-функций.

**При добавлении** элемента x: выставляем биты `h_1(x), h_2(x), ..., h_k(x)` в 1.

**При проверке** элемента y: если хоть один из k бит равен 0 — элемента точно нет (FN невозможен). Если все k бит равны 1 — элемент *вероятно* есть.

**Откуда берётся FPR:**

После добавления n элементов вероятность что конкретный бит остался нулём:

$$P(\text{бит} = 0) = \left(1 - \frac{1}{m}\right)^{kn} \approx e^{-kn/m}$$

Вероятность что все k бит случайного элемента оказались единицами (= FP):

$$\text{FPR} = \left(1 - e^{-kn/m}\right)^k$$

**Оптимальное k** — минимизируем FPR по k, берём производную и приравниваем к нулю:

$$k_{opt} = \frac{m}{n} \cdot \ln 2 \approx 0.693 \cdot \frac{m}{n}$$

При наших параметрах `m=10000, n=1000`: `k_opt = 10 × 0.693 ≈ 6.9` — именно там минимум на графике 2.

**Зависимость от m:** FPR экспоненциально зависит от m через `e^(-kn/m)`. Это объясняет почему F_m=1474 >> F_k=6.1 в ANOVA — размер массива влияет экспоненциально, количество хеш-функций — степенно.

### Наша реализация

**Хеш-функция** — polynomial rolling hash с константой Кнута:
```python
h = seed
for c in item:
    h = (h * 2654435761 + ord(c)) & 0xFFFFFFFF
return h % m
```
`2654435761` — простое число, дающее хорошее лавинообразное распределение битов. `& 0xFFFFFFFF` — обрезаем до 32 бит чтобы не выйти за пределы uint32. k разных функций = k разных seed.

**Битовый массив** — `np.zeros(m, dtype=bool)`. NumPy хранит bool как 1 байт, что в 64 раза эффективнее Python-списка объектов.

**Union через побитовое ИЛИ** — математически корректно: если элемент есть хотя бы в одном фильтре, его биты стоят в объединении.

**Intersection через побитовое И** — биты стоят только если стояли в обоих фильтрах. FPR пересечения ≤ min(FPR_1, FPR_2).

---

## 2. Counting Bloom Filter

### Почему работает математически

Заменяем каждый бит счётчиком (uint8, значения 0-255).

**add:** увеличиваем k счётчиков на 1.  
**remove:** уменьшаем k счётчиков на 1.  
**contains:** все k счётчиков > 0.

**Почему удаление корректно:** если элемент добавлен, его k счётчиков ≥ 1. После уменьшения они станут ≥ 0. Счётчики других элементов на тех же позициях не упадут до нуля (они будут уменьшены только если те элементы тоже удалены).

**Проблема переполнения:** счётчик 255 + 1 → 255 (cap). При удалении такого элемента счётчик не уменьшится → false negative появится для других элементов на той же позиции. Решение — cap at max, не уменьшать.

**FPR** та же формула что у Bloom Filter — счётчики vs биты не меняют вероятность ложного срабатывания при проверке.

**Цена удаления:** 8× больше памяти (8 бит вместо 1 бита на позицию).

### Наша реализация

```python
self.counters = np.zeros(config.m, dtype=np.uint8)  # 0-255 на позицию
self.max_count = (1 << counter_bits) - 1             # 15 при counter_bits=4

def remove(self, item):
    if item not in self:      # проверяем перед удалением
        return False          # нельзя удалить то чего нет
    for h in self.hashes:
        idx = h(item)
        if self.counters[idx] > 0:
            self.counters[idx] -= 1
```

Проверка `if item not in self` перед удалением — защита от уменьшения чужих счётчиков при попытке удалить несуществующий элемент.

---

## 3. HyperLogLog

### Почему работает математически

**Наблюдение:** если бросить монету до первого орла и выпало k решек подряд — вероятность такого `1/2^k`, значит монету бросали примерно `2^k` раз.

HLL делает то же самое с битами хеша:

1. Хешируем элемент → 64-битное число
2. Первые `p` бит → номер регистра (bucket), всего `m = 2^p` регистров
3. Оставшиеся `(64-p)` бит `w` → считаем ведущие нули `ρ(w)`
4. Регистр `M[j] = max(M[j], ρ(w) + 1)`

**Оценка кардинальности** через harmonic mean регистров:

$$\hat{n} = \alpha_m \cdot m^2 \cdot \left(\sum_{j=1}^{m} 2^{-M[j]}\right)^{-1}$$

**Почему harmonic mean а не arithmetic:** arithmetic mean чувствителен к выбросам — один регистр с большим значением исказит всё. Harmonic mean подавляет выбросы через `2^(-M[j])`.

**Поправочная константа** `α_m ≈ 0.7213` — корректирует систематическое смещение оценки, вычислена аналитически.

**Три режима:**
- Мало элементов (есть пустые регистры): `n̂ = m·ln(m/zeros)` — линейный счёт точнее
- Средний диапазон: raw estimate достаточно точен
- Много элементов (>2³²/30): поправка на переполнение 32-битного хеш-пространства

**Точность:** стандартное отклонение `σ = 1.04/√m`. При `p=14` (m=16384): `σ = 1.04/128 = 0.81%`.

**Ключевой баг который мы исправили:**
```python
# НЕПРАВИЛЬНО — считает нули в 64 битах
return 64 - w.bit_length()

# ПРАВИЛЬНО — считает нули в (64-p) битах
bits = 64 - self.p
return bits - w.bit_length()
```
`w = h >> p` — это уже `(64-p)`-битное число. Если считать нули в 64-битном пространстве, добавляем лишние p нулей сверху → регистры завышены в ~4-8 раз → оценка завышена в разы.

### Наша реализация

```python
def add(self, item):
    h = int(sha256(item.encode()).hexdigest()[:16], 16)  # 64-bit hash
    j = h & (self.m - 1)          # первые p бит = номер регистра
    w = h >> self.p                # оставшиеся (64-p) бит
    self.registers[j] = max(self.registers[j], self._clz(w) + 1)

def _clz(self, w):                 # count leading zeros
    bits = 64 - self.p
    if w == 0: return bits
    return bits - w.bit_length()
```

SHA256 вместо polynomial hash — критично для равномерного распределения по регистрам. Polynomial hash имеет корреляции в битах → некоторые регистры систематически перегружаются.

**Graph 4 объяснение:** реальная ошибка ниже теоретической `104/√m` потому что SHA256 даёт лучшее распределение чем предполагает теория (разработанная для идеальных случайных хешей).

---

## 4. Quotient Filter

### Почему работает математически

Делим хеш на две части:

$$h(x) = q \cdot 2^r + \text{rem}$$

- **Quotient** `q = h >> r` — старшие биты, используются как индекс в таблице
- **Remainder** `rem = h & (2^r - 1)` — младшие r бит, хранятся в ячейке

**FPR** зависит только от длины remainder:

$$\text{FPR} = 2^{-r}$$

При `r=6`: FPR = `1/64 ≈ 1.6%`. При `r=10`: FPR = `1/1024 ≈ 0.1%`.

**Разрешение коллизий** (два элемента с одинаковым quotient) — linear probing: идём вправо по таблице. Три флага на каждую ячейку отслеживают структуру цепочек:

- `is_occupied[q]` — в каноническом слоте q есть хотя бы один элемент с этим quotient
- `is_continuation` — ячейка продолжает цепочку предыдущего quotient
- `is_shifted` — элемент сдвинут из своей канонической позиции

**Преимущество перед Bloom Filter:** последовательный доступ к памяти при linear probing → CPU prefetcher работает эффективно → меньше cache miss → быстрее на больших данных.

### Наша реализация

```python
h = sha256(item)[:16]
q   = (h >> self.config.r) & (self.size - 1)  # индекс
rem = h & ((1 << self.config.r) - 1)           # значение для хранения
```

Linear probing при коллизии — ищем следующую свободную ячейку. Флаг `is_shifted=True` означает что элемент не на своей канонической позиции.

---

## 5. Count-Min Sketch

### Почему работает математически

Матрица счётчиков `d × w`, d независимых хеш-функций.

**add(x, count):** `table[i][h_i(x)] += count` для каждой из d строк.

**estimate(x):** `min(table[i][h_i(x)] for i in range(d))`

**Почему минимум:** каждый счётчик может быть *завышен* из-за коллизий (чужие элементы попали в ту же ячейку). Минимум по всем строкам даёт наименее загрязнённую оценку.

**Гарантия ошибки:**

$$P\left(\hat{f}(x) \leq f(x) + \varepsilon N\right) \geq 1 - \delta$$

где `ε = e/w`, `δ = (1/2)^d`, `N` — суммарный поток, `e ≈ 2.718`.

Пример при `w=1000, d=5`:
- `ε = 2.718/1000 = 0.27%` от суммарного потока
- `δ = (0.5)^5 = 3%` — вероятность превышения этой ошибки

**Оценка всегда ≥ реальной:** коллизии только завышают счётчики, никогда не занижают.

**Merge через сложение:** `merged.table = table1 + table2`. Математически корректно — счётчики аддитивны.

### Наша реализация

```python
self.table = np.zeros((depth, width), dtype=np.uint32)

def add(self, item, count=1):
    for i, h in enumerate(self.hashes):
        self.table[i, h(item)] += count

def estimate(self, item):
    return min(self.table[i, h(item)] for i, h in enumerate(self.hashes))
```

`dtype=np.uint32` — счётчики до 4 миллиардов, достаточно для большинства задач.

**Graph 6 объяснение:** при `width=100` на 100 уникальных элементов — в среднем 1 элемент на ячейку, но из-за birthday paradox коллизии частые → ошибка ~11%. При `width=500` коллизии редки → ошибка ~0%.

---

## 6. MinHash

### Почему работает математически

**Jaccard similarity** двух множеств:

$$J(A, B) = \frac{|A \cap B|}{|A \cup B|}$$

**Теорема MinHash:**

$$P\left(\min_{x \in A} h(x) = \min_{x \in B} h(x)\right) = J(A, B)$$

**Доказательство:** рассмотрим `A ∪ B`. Применяем хеш-функцию h ко всем элементам. Минимальный хеш принадлежит элементу из `A ∩ B` (= общий для обоих) с вероятностью `|A∩B|/|A∪B|` — именно Jaccard similarity.

**Сигнатура из k хеш-функций:**

$$\text{sig}(A) = [\min_{x \in A} h_1(x),\ \min_{x \in A} h_2(x),\ \ldots,\ \min_{x \in A} h_k(x)]$$

**Оценка Jaccard:**

$$\hat{J}(A,B) = \frac{|\{i : \text{sig}(A)[i] = \text{sig}(B)[i]\}|}{k}$$

**Точность:** стандартное отклонение оценки `σ = √(J(1-J)/k)`. При `k=256` и `J=0.5`: `σ = √(0.25/256) ≈ 3%`.

### Наша реализация

```python
self.signature = np.full(num_perm, np.inf)  # стартуем с +∞

def update(self, item):
    for i, h in enumerate(self.hashes):
        self.signature[i] = min(self.signature[i], h(item))
        # берём минимум по всем элементам множества

def jaccard(self, other):
    return np.mean(self.signature == other.signature)
    # доля совпавших минимумов = оценка Jaccard
```

Стартуем с `np.inf` потому что ищем минимум — любой реальный хеш будет меньше бесконечности.

**Graph 5 объяснение:** при 0% пересечении `J=0`, минимумы никогда не совпадают → оценка ~0. При 100% пересечении `J=1`, минимумы всегда совпадают → оценка ~1. Небольшое расхождение при 75-90% — статистическая погрешность `≈ 1/√256 ≈ 6%`.

---

## ANOVA — интерпретация результатов

**Что считаем:**

```
F = (дисперсия МЕЖДУ группами) / (дисперсия ВНУТРИ групп)
```

Группируем 30 измерений FPR по значению фактора:

```
Группа m=1000:  [0.31, 0.29, 0.33, ...]  → среднее 0.31
Группа m=5000:  [0.04, 0.05, 0.04, ...]  → среднее 0.04
Группа m=10000: [0.01, 0.01, 0.01, ...]  → среднее 0.01
```

Если группы сильно различаются между собой (большая межгрупповая дисперсия) относительно разброса внутри каждой группы → F большой → фактор влияет.

**Наши результаты:**
```
F_m = 1474,  p < 0.001  ***
F_k = 6.1,   p < 0.001  ***
```

`F_m/F_k = 241` → m влияет в 241 раз сильнее чем k.

`p < 0.001` для обоих → вероятность получить такой F случайно (при условии что фактор не влияет) меньше 0.1%.

**Физический смысл:** m влияет через экспоненту `e^(-kn/m)` — изменение m сдвигает весь показатель степени. k влияет через степень `(...)^k` — изменение k меняет только множитель. Экспонента всегда чувствительнее степени → m доминирует.

**Вывод для проектирования системы:** если нужно снизить FPR — сначала увеличивай m, потом настраивай k. Увеличение k без увеличения m даёт минимальный эффект.